JMETER  Elasticsearch Tests 
---------------------------------------

Performs ingestion, query , scroll queries at specified throughput and logs latencies on a CSV file 


Pre-requisites
-------------------

Install Jmeter 3.0
Have a ES running cluster accessible

If planning to use the setup.bash script you need :
1.  npm
2.  coffeescript (see http://coffeescript.org/)


If planning to perform scan and scroll queries from Jmeter :
3. you need python 


Setup data and queries 
-------------------------------
* the ./input folder contains bulk indexing JSON files that can be generated by running the setup.bash (as a demo) That's where you need to put your bulk requests files that will be used by JMeter.
* If you want jmeter to send queries, there should be a query input csv (see format below) in ./queries
* If you want jmeter to sens scroll queries, there should be a scroll input csv (see format below) in ./queries

vars.bash
---------------
Modify the test parameters in the script as follows :

JMETER_PATH : The path of your JMeter install
USER= Shield User
PASS= Shield Pass
HOST= Target Elastic hostname/ IP address
PORT= Target Elastic port (ex : 9200)
INDEX= Indices being queried (ex: apachelogs-* )
QUERY_CSV= the file containing queries input in relative to the csv folder  (ex:input1K1h.csv)
SCROLL_CSV=the file containing scroll input   relative to the csv folder (ex:inputScroll.csv) 
SHIELD_ENABLED= is shied enabled
DOCS_PER_BULK= How many events on each bulk request
NB_FILES=Nb of bulk request to generate. set -1 to generate all bulks for 300MB raw data.


setup.bash
---------------
Use setup.bash script to setup test data from the sample apache logs located in ./ingest/logs.json.gz
This script will generate in ./input 20 BULK requests with 500 docs in each request.
Tweak the parameters in the script to create more bulks / more docs per bulk



cleanup.bash
------------------
This script cleans up all files generated by the tests including results.csv


test.bash
------------

Set the required parameters for the test :

 * Througput in requests per minute
QUERY_THROUGHPUT=5.0
SCROLL_THROUGHPUT=20.0

* Ingestion Througput in raw MB/s .
This throughput takes is currentely based on the avg size of docs in one file in the input folder
INGEST_THROUGHPUT=1

* test tag : include a tag on each result line for this test
TEST_TAG=T0602

Run this script to perfome  the JMETER test


elk_stress.jmx runs in the background and takes as additional params :
  -JtestScroll=true/false   : wether we enable the scroll queries to run during the test
  -JtestIngest=true/false   :  ""  ""     ""       ingestion "" "" 
  -JtestQuery=true/false    :   ""  ""    ""        querying 


* if ingestion enabled :
JMeter will iterate the files in ./input (for ever) and send the bulk queries at specified throughput

* if scroll enabled :
JMeter will iterate the scroll CSV (for ever) and send scroll queries at specified throughput  

* if query enabled :
JMeter will iterate the scroll CSV  (for ever) and send queries  at specified throughput  

 if shield not used then you have to disable the authentication managers in the Jmeter test plan (using a client).  not tested  with an authentication manager without shield


* Format of the query input CSV :
elk_stress.jmx comes with a generic ES query sampler. This query sampler takes as an input a line in the QUERY_CSV file and inserts each value  in the corresponding query. json body

example (3 lines):

time1,time2,country_code,queryFileName
440772151510,1440775751510,,query1.json
1441972718913,1441976318913,,query1.json
,,US,query2.json


Note the variables time1, time2 referenced in the corresponding query1.json and country_code in query2.json

You can refer to multiple queries in the CSV.. Make sure the CSV headers properly match each CSV values on each rows , ex:

Each query will be sent iteratively by JMeter, and the global throughput will be  QUERY_THROUGHPUT
 
* genDateIntervals.coffee can be used to generate random timestamp intervals.
supported parameters: 

date1 = date min 
date2 = date max 
interval = width of the interval
unit = time unit  (s,m,h,d)
nbSamples = number of intervals to generate


  
Test results
---------------
Are located in results/results.csv
the latency in ms is the csv file

